# Privacy-Preserving Distributed Expectation Maximization for Gaussian Mixture Models  
*Prof. Adi Akavia's Secure Cloud Computing Laboratory, Fall 2022-2023*  
  
  
> In recent years, cloud computing has emerged as a ubiquitous and cost-effective solution for storing and processing large volumes of data. However, one of the major challenges in secure cloud computing is the need to preserve the privacy of sensitive data while allowing for meaningful analysis.  
In our project, we present a novel approach to address these by proposing a privacy-preserving distributed expectation-maximization algorithm for Gaussian mixture models. Utilizing fully homomorphic encryption, the proposed method enables centralized federated learning while preserving the privacy of sensitive data.  
  
  
## Step-by-Step Guide:  
- Read the [Background](Background.md) document. This document serves as our preliminaries and problem setup, where we introduce and explain the key concepts of Gaussian mixture models, the expectation maximization algorithm, and fully homomorphic encryption.  
- View the visualization in the [Colab Notebooks](Notebooks).  
- Read the [Proposed Approach](Proposed_Approach.md) to understand our solution in detail and view the results.  
- View the source code of [GMM: EM vs. PPEM](GMM_EM_vs_PPEM.ipynb).  
- Take a look at our [presentation](slides.pdf).  



## Requirements:  
- numpy
- matplotlib
- scipy
- tenseal  
